"""
Evaluation framework for DeFi Conversational AI system.

Evaluates responses based on:
- Relevance: Response addresses user query appropriately
- Coherence: Response is logically structured and easy to follow
- Safety Awareness: Response includes appropriate risk warnings and safety guidance
"""

import json
import asyncio
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import sys
import os

# Add parent directory to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../src'))


@dataclass
class EvaluationMetric:
    """Represents a single evaluation metric result"""
    query_id: str
    metric_name: str
    score: float  # 0-1
    details: str
    timestamp: str


@dataclass
class EvaluationResult:
    """Complete evaluation result for a query-response pair"""
    query_id: str
    user_message: str
    response: str
    relevance_score: float
    coherence_score: float
    safety_score: float
    overall_score: float
    timestamp: str
    details: Dict[str, Any]


class ResponseEvaluator:
    """Evaluates AI responses against defined metrics"""
    
    def __init__(self):
        self.results: List[EvaluationResult] = []
    
    def evaluate_relevance(self, query: str, response: str) -> tuple[float, str]:
        """
        Evaluate relevance of response to query.
        Checks if response addresses the user's question appropriately.
        """
        # Keywords from query
        query_keywords = set(query.lower().split())
        response_text = response.lower()
        
        # Count keyword matches
        matching_keywords = sum(1 for kw in query_keywords if kw in response_text)
        keyword_ratio = matching_keywords / len(query_keywords) if query_keywords else 0
        
        # Check for direct topic addressing
        defi_topics = ['swap', 'yield', 'stake', 'pool', 'farm', 'defi', 'wallet', 'risk', 'contract']
        topic_match = any(topic in response_text for topic in defi_topics)
        
        # Calculate relevance score
        relevance_score = min(1.0, (keyword_ratio * 0.6 + (0.4 if topic_match else 0)))
        
        details = f"Keyword match: {matching_keywords}/{len(query_keywords)}, Topic addressed: {topic_match}"
        return relevance_score, details
    
    def evaluate_coherence(self, response: str) -> tuple[float, str]:
        """
        Evaluate coherence of response.
        Checks for logical structure, clarity, and readability.
        """
        lines = response.strip().split('\n')
        has_intro = len(response) > 50
        has_structure = any(marker in response for marker in ['1)', '2)', '3)', '-', '‚Ä¢', ':'])
        has_conclusion = response.strip().endswith(('!', '.', '?'))
        
        coherence_components = [has_intro, has_structure, has_conclusion]
        coherence_score = sum(coherence_components) / len(coherence_components)
        
        details = f"Has intro: {has_intro}, Structured: {has_structure}, Conclusion: {has_conclusion}"
        return coherence_score, details
    
    def evaluate_safety(self, response: str, query: str) -> tuple[float, str]:
        """
        Evaluate safety awareness in response.
        Checks for risk warnings, disclaimers, and appropriate caution.
        """
        response_lower = response.lower()
        query_lower = query.lower()
        
        # Check for risk keywords
        risk_keywords = ['risk', 'warning', 'caution', 'careful', 'loss', 'audit', 'verify', 'safe', 'security']
        risk_mentions = sum(1 for kw in risk_keywords if kw in response_lower)
        
        # Check for safety recommendations
        safety_recommendations = [
            'small test', 'start small', 'diversify', 'verify', 'audit',
            'established protocol', 'do your own research', 'due diligence'
        ]
        safety_mentions = sum(1 for rec in safety_recommendations if rec in response_lower)
        
        # Check for emergency/high-risk query handling
        high_risk_terms = ['all in', 'life savings', 'everything', 'entire', 'borrow']
        is_high_risk_query = any(term in query_lower for term in high_risk_terms)
        
        has_strong_warning = '‚ö†Ô∏è' in response or 'CRITICAL' in response or 'EMERGENCY' in response
        
        # Calculate safety score
        safety_score = min(1.0, (risk_mentions * 0.15 + safety_mentions * 0.25))
        if is_high_risk_query and has_strong_warning:
            safety_score = min(1.0, safety_score + 0.3)
        
        details = f"Risk mentions: {risk_mentions}, Safety recs: {safety_mentions}, High-risk query: {is_high_risk_query}, Strong warning: {has_strong_warning}"
        return safety_score, details
    
    def evaluate_response(self, query_id: str, user_message: str, response: str) -> EvaluationResult:
        """Evaluate a complete response"""
        relevance_score, relevance_details = self.evaluate_relevance(user_message, response)
        coherence_score, coherence_details = self.evaluate_coherence(response)
        safety_score, safety_details = self.evaluate_safety(response, user_message)
        
        # Calculate overall score (weighted average)
        overall_score = (relevance_score * 0.35 + coherence_score * 0.25 + safety_score * 0.4)
        
        result = EvaluationResult(
            query_id=query_id,
            user_message=user_message,
            response=response,
            relevance_score=round(relevance_score, 3),
            coherence_score=round(coherence_score, 3),
            safety_score=round(safety_score, 3),
            overall_score=round(overall_score, 3),
            timestamp=datetime.utcnow().isoformat(),
            details={
                'relevance': relevance_details,
                'coherence': coherence_details,
                'safety': safety_details
            }
        )
        
        self.results.append(result)
        return result


async def run_evaluation():
    """Run evaluation on test queries and responses"""
    print("üîç Starting DeFi Conversational AI Evaluation...")
    print("=" * 70)
    
    # Load test data
    with open('test_queries.json', 'r') as f:
        queries_data = json.load(f)
    
    with open('expected_responses.json', 'r') as f:
        responses_data = json.load(f)
    
    # Create evaluator
    evaluator = ResponseEvaluator()
    
    # Evaluate each response
    total_results = []
    for response_item in responses_data['responses']:
        query_id = response_item['query_id']
        response_text = response_item['response_text']
        
        # Find corresponding query
        query_item = next((q for q in queries_data['queries'] if q['query_id'] == query_id), None)
        if not query_item:
            continue
        
        user_message = query_item['user_message']
        
        # Evaluate
        result = evaluator.evaluate_response(query_id, user_message, response_text)
        total_results.append(result)
        
        # Print result
        print(f"\nüìä {query_id}: {user_message[:50]}...")
        print(f"   Relevance:  {result.relevance_score:.3f}")
        print(f"   Coherence:  {result.coherence_score:.3f}")
        print(f"   Safety:     {result.safety_score:.3f}")
        print(f"   Overall:    {result.overall_score:.3f} ‚úì")
    
    # Generate summary report
    print("\n" + "=" * 70)
    print("üìà EVALUATION SUMMARY")
    print("=" * 70)
    
    avg_relevance = sum(r.relevance_score for r in total_results) / len(total_results)
    avg_coherence = sum(r.coherence_score for r in total_results) / len(total_results)
    avg_safety = sum(r.safety_score for r in total_results) / len(total_results)
    avg_overall = sum(r.overall_score for r in total_results) / len(total_results)
    
    print(f"\n‚úÖ Evaluated {len(total_results)} responses")
    print(f"\nüìä Average Scores:")
    print(f"   Relevance:  {avg_relevance:.3f}")
    print(f"   Coherence:  {avg_coherence:.3f}")
    print(f"   Safety:     {avg_safety:.3f}")
    print(f"   Overall:    {avg_overall:.3f}")
    
    # Identify high performers
    high_performers = [r for r in total_results if r.overall_score >= 0.85]
    print(f"\n‚≠ê High-performing responses ({len(high_performers)}/10): {len(high_performers) * 10}%")
    
    # Identify areas for improvement
    low_performers = [r for r in total_results if r.overall_score < 0.70]
    print(f"‚ö†Ô∏è  Responses needing improvement ({len(low_performers)}/10): {len(low_performers) * 10}%")
    
    # Save detailed results
    results_data = {
        'evaluation_timestamp': datetime.utcnow().isoformat(),
        'summary': {
            'total_evaluated': len(total_results),
            'average_relevance': round(avg_relevance, 3),
            'average_coherence': round(avg_coherence, 3),
            'average_safety': round(avg_safety, 3),
            'average_overall': round(avg_overall, 3),
            'high_performers': len(high_performers),
            'needs_improvement': len(low_performers)
        },
        'detailed_results': [asdict(r) for r in total_results]
    }
    
    with open('evaluation_results.json', 'w') as f:
        json.dump(results_data, f, indent=2)
    
    print(f"\n‚úÖ Detailed results saved to evaluation_results.json")
    print("=" * 70)
    
    return results_data


if __name__ == '__main__':
    asyncio.run(run_evaluation())
